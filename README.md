# Sign-Language-to-Text-with-Auto-complete

Deaf and mute(D&M) people usually make use of a vision based medium for
communication; this is usually some form of sign language. But most people
who do not rely on sign language for communication are not aware of how
to read or use it. The aim of this project is to create a gesture classication
system that can classify images among the 26 letters of the English alphabet.
This will enable communication between normal and D&M people a lot sim-
pler. By using our system, even those who don't know any Sign Language
can understand it. Our system will convert American Sign Language sym-
bols to English letters in real-time.
4 Motivation


![image](https://user-images.githubusercontent.com/60578239/152095544-8020a709-378d-48cf-884b-6b71df745cd6.png)


- Implementation in python using OpenCV, Tensorflow, and Keras
- It uses CNN to train the required models for prediction using the VGG16 model
- Prediction of sign language symbols with 96% accuracy
